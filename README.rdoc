= Under Construction

We are currently using this but are in the process of cleaning it up to release it to the public.  It should be ready in a week or two.


= Motivation

We are restricted in our team that we don't always get to chose the hosting provider and each project always has different requirements.  The failover systems out there seem to ask "set your system up like this and then you can use this failover solution."  We don't always have that luxury but we'll still be responsible if something goes down.  So we wrote this to adapt to the various systems we have.  It can do a failover in a manner that would be considered a best practice, or it can do a failover in a more hacky method because that's your only option.  And if there are any additional tasks that you need done before or after a failover, it can do those too.  And the whole thing can report to Nagios.


= Goals

* Failover via /etc/hosts, VIP, or someother means we haven't thought of yet.
* Restart applications of various forms (Rails apps, job server).
* Perform or reset the failover manually from the command line.
* Mix and match monitoring and failover plugins.
* Monitor things well enough that it will know if something changed in the system that would prevent it from doing it's job.
* Accurately report system status to Nagios.
* Provide a full system status breakdown much like MogileFS does via the command line.


= Overview

== Plugin Architecture

Since there are guaranteed to be edge cases that hundreds of developers have that I don't have, this has a straight-forward mechanism to develop plugins and drop them in a config directory and use them right along side the built in stuff.

Suppose you're have a job server or a search server that doesn't work with the existing monitor.  You could write a simple monitor for it, specify it in the pool config and use the existing Failover Protocols to perform the failover.  If there is some special task that you need done that the existing failover protocols don't support you can write your own in the same manner as the monitor.  Examples for these are created for you by the deadpool_generate_config command.

== Chainable Failover Protocols

Chainable failover protocols means you can take two or more Failover Protocols and have them execute in succession in the event of a failover.  For example, hang maintenance page, shutdown job server, wait for replication to catch up, preform the switch, restart the web servers, start the job server, remove the maintenance page.


== Multiple Services

Multiple services (ex. mysql_development, mysql_staging, redis) can be configured under a single instance and multiple instances can be configured to run on a single box. We did this so you could run your staging and production deadpool servers on the same box without having to use the same versions of either deadpool or your custom plugins.  Ergo you can develop in house plugins and deploy them to staging on the monitoring server without risking production.


== Monitoring

There are lots of moving parts.  Since a failover system is worthless if you don't know when it's down or when something has changed that would prevent it from working the first design decision was that the whole system had to be able to test it's status and report it.  Probably half the code in this gem is just to monitor itself.  Deadpool can test each link in the chain and report when something is out of place.  Meaning it tests more than MySQL, it tests that all the app servers are pointing at the correct database and that it has write permission on the file it would have to change and such.


    $ deadpool_admin --nagios_report
    OK -  last checked 12 seconds ago.


    $ deadpool_admin --full_report
    System Status: OK

    Deadpool::Server
    OK - checked 2 seconds ago.

      Deadpool::Handler - staging_database
      OK - checked 3 seconds ago.
      Primary Check OK.

        Deadpool::Monitor::Mysql - staging_database
        OK - checked 2 seconds ago.
        Primary and Secondary are up.

        Deadpool::FailoverProtocol::EtcHosts - staging_database
        OK - checked 2 seconds ago.
        Write check passed all servers: 10.1.2.3, 10.1.2.4
        All client hosts are pointed at the primary.

        Deadpool::FailoverProtocol::ExecRemoteCommand - staging_database
        OK - checked 1 seconds ago.
        Exec test passed all servers: 10.1.2.3, 10.1.2.4

      Deadpool::Handler - dev_database
      OK - checked 3 seconds ago.
      Primary Check OK.

        Deadpool::Monitor::Mysql - dev_database
        OK - checked 1 seconds ago.
        Primary and Secondary are up.

        Deadpool::FailoverProtocol::EtcHosts - dev_database
        OK - checked 0 seconds ago.
        Write check passed all servers: 10.1.2.3, 10.1.2.4
        All client hosts are pointed at the primary.

        Deadpool::FailoverProtocol::ExecRemoteCommand - dev_database
        OK - checked 0 seconds ago.
        Exec test passed all servers: 10.1.2.3, 10.1.2.4





== How it works

It periodically checks that the primary is okay at an interval of your choosing.  When the primary check has failed enough times in a row to exceed a threshold of your choosing it will execute the failover protocol.  The failover protocol is just a list of failover protocols in order.  Generally each one will perform a preflight check first.  As each one finishes the failover it records it's state and success or failure.  Once it's all done, deadpool locks the state so an admin can see what happened and if there were any issues along the way.

Deadpool is a single use tool.  Once it's failed over, it's done.  It can perform a manual promotion from the command line but it will have to be restarted to work again.  It does not attempt to fail back over if the primary comes back online.  That just sounds way too risky.




= Installing/Setup

    $ gem install deadpool
    $ deadpool_generate_config --path=/etc/deadpool


== /etc/hosts method






